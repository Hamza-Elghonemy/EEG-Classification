{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    raw=mne.io.read_raw_gdf(path,preload=True,\n",
    "                          eog=['EOG-left', 'EOG-central', 'EOG-right'])\n",
    "    raw.drop_channels(['EOG-left', 'EOG-central', 'EOG-right'])\n",
    "    raw.set_eeg_reference()\n",
    "    events=mne.events_from_annotations(raw)\n",
    "    epochs = mne.Epochs(raw, events[0], event_id=[7,8,9,10],on_missing ='warn')\n",
    "    labels=epochs.events[:,-1]\n",
    "    features=epochs.get_data()\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "features,labels,groups=[],[],[]\n",
    "for i in range(1,10):\n",
    "  feature,label=read_data(f'data/A0{i}T.gdf')\n",
    "  features.append(feature) # append the features of eeg data\n",
    "  labels.append(label) # append the labels of the eeg data\n",
    "  groups.append([i]*len(label)) # append the group number of the eeg data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2448, 22, 176), (2448,), (2448,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=np.concatenate(features)\n",
    "labels=np.concatenate(labels)\n",
    "groups=np.concatenate(groups)\n",
    "\n",
    "features.shape,labels.shape,groups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2647058823529412,\n",
       " 0.2647058823529412,\n",
       " 0.23529411764705882,\n",
       " 0.23529411764705882)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(labels==7),np.mean(labels==8),np.mean(labels==9),np.mean(labels==10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(648, 22)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(features[labels==7],axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2448, 352)\n"
     ]
    }
   ],
   "source": [
    "# Statistical Analysis\n",
    "# Get the statistical values of the features\n",
    "x_features = []\n",
    "for feature in features:\n",
    "    stats_list = [\n",
    "        np.mean(feature, axis=-1),\n",
    "        np.std(feature, axis=-1),\n",
    "        np.max(feature, axis=-1),\n",
    "        np.min(feature, axis=-1),\n",
    "        np.median(feature, axis=-1),\n",
    "        np.var(feature, axis=-1),\n",
    "        np.ptp(feature, axis=-1),\n",
    "        stats.skew(feature, axis=-1),\n",
    "        stats.kurtosis(feature, axis=-1),\n",
    "        stats.iqr(feature, axis=-1),\n",
    "        stats.mode(feature, axis=-1)[0],\n",
    "        stats.sem(feature, axis=-1),\n",
    "        np.argmax(feature, axis=-1),\n",
    "        np.argmin(feature, axis=-1),\n",
    "        np.sum(np.abs(np.diff(feature, axis=-1)),axis=-1),\n",
    "        np.sqrt(np.mean(feature**2, axis=-1))\n",
    "    ]\n",
    "    # Flatten each statistic to ensure they are 1D arrays\n",
    "    x_features.append(np.concatenate(stats_list))\n",
    "x_features = np.array(x_features)\n",
    "print(x_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "352/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 0.1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3174768518518518"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pipeline\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "# Create a parameter grid\n",
    "param_grid = {\n",
    "    'clf__C': [0.1, 0.5,0.7, 1, 3,5,7, 10, 50, 100],\n",
    "}\n",
    "\n",
    "# Create a group k-fold object\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=gkf, scoring='accuracy',n_jobs= 12)\n",
    "\n",
    "# Fit the grid search object\n",
    "grid_search.fit(x_features, labels, groups=groups)\n",
    "\n",
    "# Get the best parameters\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Get the best score\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=5)\n",
    "models = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=500))\n",
    "    ]),\n",
    "    'SVC': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', SVC())\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ]),\n",
    "    'Gradient Boosting': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', GradientBoostingClassifier())\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'clf__C': [0.1, 0.5, 1, 5, 10]\n",
    "    },\n",
    "    'SVC': {\n",
    "        'clf__C': [0.1, 1, 10],\n",
    "        'clf__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'clf__n_estimators': [50, 100],\n",
    "        'clf__max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'clf__n_estimators': [50, 100],\n",
    "        'clf__learning_rate': [0.01, 0.1, 1]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best for Logistic Regression: Score = 0.3174768518518518, Params = {'clf__C': 0.1}\n",
      "Best for SVC: Score = 0.3108796296296297, Params = {'clf__C': 0.1, 'clf__kernel': 'linear'}\n",
      "Best for Random Forest: Score = 0.2929398148148148, Params = {'clf__max_depth': 10, 'clf__n_estimators': 100}\n",
      "Best for Gradient Boosting: Score = 0.2982638888888889, Params = {'clf__learning_rate': 0.1, 'clf__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "best_scores = {}\n",
    "for model_name, pipeline in models.items():\n",
    "    grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=gkf, scoring='accuracy', n_jobs=12)\n",
    "    grid_search.fit(x_features, labels, groups=groups)\n",
    "    best_score = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best for {model_name}: Score = {best_score}, Params = {best_params}\")\n",
    "    best_scores[model_name] = best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
